{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirandaday16/hin_urd_translation/blob/master/Copy_of_LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWB9udaKesP",
        "colab_type": "text"
      },
      "source": [
        "Your task is to train *character-level* language models. \n",
        "You will train unigram, bigram, and trigram character level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHFJmuftHJld",
        "colab_type": "code",
        "outputId": "f35ed685-7b3e-4923-ed71-61f7dd6b416c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import pandas as pd\n",
        "import httpimport\n",
        "import collections\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U0UCuyHQkai",
        "colab_type": "text"
      },
      "source": [
        "This code loads the training and test data. Each dataset is a list of books. Each book contains a list of sentences, and each sentence contains a list of words. For building a character language model, you should join the words of a sentence together with a space character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0pfuiEChTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAO9VjFLArq",
        "colab_type": "text"
      },
      "source": [
        "## 1.1\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n",
        "\n",
        "If your machine takes a long time to perform this computation, you may save these counts to files in your github repository and load them on request. This is not necessary, however."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4VOoiSIoUF",
        "colab_type": "code",
        "outputId": "6f1846ca-5a3e-4528-c188-37caeda257b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Collect unigram character counts stats (list of char counts, avg char count)\n",
        "def get_unigram_char_counts():\n",
        "  char_counts = []\n",
        "  for document in train:\n",
        "    for sentence in document:\n",
        "      for word in sentence:\n",
        "        char_counts.append(len(word))\n",
        "  return char_counts\n",
        "\n",
        "def get_unigram_char_avg(char_counts_list):\n",
        "  avg = 0\n",
        "  unigram_count = 0\n",
        "  for count in char_counts_list:\n",
        "    avg += count\n",
        "    unigram_count += 1\n",
        "  avg = avg / unigram_count\n",
        "  return avg\n",
        "\n",
        "# Collect bigram character counts stats (list of char counts, avg char count)\n",
        "def get_bigram_list():\n",
        "  bigrams_list = []\n",
        "  for document in train:\n",
        "    for sentence in document:\n",
        "      for i in range(len(sentence) - 1):\n",
        "        bigrams_list.append([sentence[i], sentence[i + 1]])\n",
        "  return(bigrams_list)\n",
        "\n",
        "def get_bigram_char_counts(bigrams_list):\n",
        "  char_counts = []\n",
        "  for bigram in bigrams_list:\n",
        "    bigram_char_count = len(bigram[0]) + len(bigram[1])\n",
        "    char_counts.append(bigram_char_count)\n",
        "  return char_counts\n",
        "\n",
        "def get_bigram_char_avg(char_counts_list):\n",
        "  avg = 0\n",
        "  bigram_count = 0\n",
        "  for count in char_counts_list:\n",
        "    avg += count\n",
        "    bigram_count += 1\n",
        "  avg = avg / bigram_count\n",
        "  return avg\n",
        "\n",
        "# Collect trigram character counts stats (list of char counts, avg char count)\n",
        "def get_trigram_list():\n",
        "  trigrams_list = []\n",
        "  for document in train:\n",
        "      for sentence in document:\n",
        "        for i in range(len(sentence) - 2):\n",
        "          trigrams_list.append([sentence[i], sentence[i + 1], sentence[i + 2]])\n",
        "  return(trigrams_list)\n",
        "\n",
        "def get_trigram_char_counts(trigrams_list):\n",
        "  char_counts = []\n",
        "  for trigram in trigrams_list:\n",
        "    trigram_char_count = len(trigram[0]) + len(trigram[1]) + len(trigram[2])\n",
        "    char_counts.append(trigram_char_count)\n",
        "  return char_counts\n",
        "\n",
        "def get_trigram_char_avg(char_counts_list):\n",
        "  avg = 0\n",
        "  trigram_count = 0\n",
        "  for count in char_counts_list:\n",
        "    avg += count\n",
        "    trigram_count += 1\n",
        "  avg = avg / trigram_count\n",
        "  return avg\n",
        "\n",
        "\n",
        "def main():\n",
        "  print(\"Character Count Statistics for ngrams\")\n",
        "  print(\"UNIGRAMS\")\n",
        "  print(\"Average unigram character count: \", get_unigram_char_avg(get_unigram_char_counts()))\n",
        "  print(\"BIGRAMS\")\n",
        "  print(\"Average bigram character count: \", get_bigram_char_avg(get_bigram_char_counts(get_bigram_list())))\n",
        "  print(\"TRIGRAMS\")\n",
        "  print(\"Average trigram character count: \", get_trigram_char_avg(get_trigram_char_counts(get_trigram_list())))\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character Count Statistics for ngrams\n",
            "UNIGRAMS\n",
            "Average unigram character count:  3.6186308183165288\n",
            "BIGRAMS\n",
            "Average bigram character count:  7.366206767270403\n",
            "TRIGRAMS\n",
            "Average trigram character count:  11.087873364372285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3mnaIvQnhI",
        "colab_type": "text"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using linear interpolation smoothing method. For determining λs for linear interpolation, you can divide the training data into a new training set (80%) and a held-out set (20%), then using grid search method:\n",
        "Choose ~10 values of λ to test using grid search on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
        "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF4HhQGOZD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "eb53951a-aa92-42c5-9126-194c6cffbfbe"
      },
      "source": [
        "# Tokenize entire dataset\n",
        "tokenized_train = []\n",
        "for document in train:\n",
        "  for sentence in document:\n",
        "    for word in sentence:\n",
        "      tokenized_train.append(word)\n",
        "# print(len(tokenized_train))\n",
        "# Contains 2,621,785 tokens\n",
        "\n",
        "# Split training set into training (80%) and held-out (20%) sets\n",
        "new_training = []\n",
        "held_out = []\n",
        "\n",
        "# print(len(train))\n",
        "# length of train set is 18 documents; 80% is approx. 14\n",
        "\n",
        "new_training = train[0:14]\n",
        "held_out = train[14:]\n",
        "\n",
        "# new_training contains 14 documents (~78%), and held_out contains 4 (~22%).\n",
        "\n",
        "# Creating a unigram language model to generate perplexities\n",
        "def create_unigram_model():\n",
        "  unigram_model = collections.defaultdict(lambda: 0.01)\n",
        "  for token in tokenized_train:\n",
        "    try:\n",
        "      unigram_model[token] += 1\n",
        "    except KeyError:\n",
        "      unigram_model[token] = 1\n",
        "      continue\n",
        "    num = float(sum(unigram_model.values()))\n",
        "    print(unigram_model)\n",
        "    for word in unigram_model:\n",
        "      unigram_model[word] = unigram_model[word]/num\n",
        "    return unigram_model\n",
        "\n",
        "# # Calculate perplexity for each document using linear interpolation smoothing.\n",
        "def calculate_perplexity(document_number, unigram_model):\n",
        "  tokenized_document = []\n",
        "  for sentence in new_training[document_number]:\n",
        "    for word in sentence:\n",
        "      tokenized_document.append(word)\n",
        "  # print(tokenized_document[:10])\n",
        "  perplexity = 1\n",
        "  N = 0\n",
        "  for unigram in tokenized_document:\n",
        "    N += 1\n",
        "    perplexity = perplexity * (1/unigram_model[unigram])\n",
        "    # print(unigram_model[unigram])\n",
        "  perplexity = (perplexity)**(1/float(N))\n",
        "  return perplexity\n",
        "\n",
        "\n",
        "# def sort_by_perplexity():\n",
        "\n",
        "calculate_perplexity(1, create_unigram_model())\n",
        "# create_unigram_model()\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function create_unigram_model.<locals>.<lambda> at 0x7f34a5b5e2f0>, {'[': 1.1})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQl2u_giVW5e",
        "colab_type": "text"
      },
      "source": [
        "## 1.3\n",
        "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUTEk8QUehL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trigram language model with add-lambda smoothing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhXTB5TXR25",
        "colab_type": "text"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFq1ECgDI6QG",
        "colab_type": "text"
      },
      "source": [
        "[Your text here.]"
      ]
    }
  ]
}